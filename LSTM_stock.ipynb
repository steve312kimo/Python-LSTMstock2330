{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "from plotly.offline import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, sequence_length=10, split=0.8):\n",
    "    data_all = np.array(df).astype(float)    # 轉為浮點型別矩陣\n",
    "    #print(data_all.shape) # (241,1)\n",
    "    scaler = MinMaxScaler() #數據標準化\n",
    "    data_all = scaler.fit_transform(data_all)  # 將數據縮放為 0~1 之間\n",
    "    data = []\n",
    "    # data 資料共有 (245-10-1)=234 筆\n",
    "    for i in range(len(data_all) - sequence_length - 1):\n",
    "        # 每筆 data 資料有 11 欄\n",
    "        data.append(data_all[i: i + sequence_length + 1])\n",
    "    reshaped_data = np.array(data).astype('float64')\n",
    "\n",
    "    x = reshaped_data[:, :-1] # 第 1至第10個欄位為 特徵\n",
    "    y = reshaped_data[:, -1]  # 第 11個欄位為 label\n",
    "    #print(x.shape,y.shape) # (230,10,1) (230,1)\n",
    "    split_boundary = int(reshaped_data.shape[0] * split)\n",
    "    train_x = x[: split_boundary] # 前 80% 為 train 的特徵\n",
    "    test_x = x[split_boundary:]   # 最後 20% 為 test 的特徵\n",
    " \n",
    "    train_y = y[: split_boundary] # 前 80% 為 train 的 label\n",
    "    test_y = y[split_boundary:]   # 最後 20% 為 test 的 label\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, scaler, x, y, reshaped_data\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()     \n",
    "    # 隱藏層：256 個神經元，input_shape：(10,1)\n",
    "    # TIME_STEPS=10,INPUT_SIZE=1\n",
    "    model.add(LSTM(input_shape=(10,1),units=256,unroll=False))\n",
    "    model.add(Dense(units=1)) # 輸出層：1 個神經元\n",
    "    #compile:loss, optimizer, metrics\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(train_x, train_y, test_x, test_y):\n",
    "    model = build_model()\n",
    "    try:\n",
    "        model.fit(train_x, train_y, batch_size=100, epochs=300, validation_split=0.1)\n",
    "        predict = model.predict(test_x)\n",
    "        predict = np.reshape(predict, (predict.size, )) #轉換為1維矩陣\n",
    "    except KeyboardInterrupt:\n",
    "        print(predict)\n",
    "        print(test_y)\n",
    "    return predict, test_y #傳回 預測值和真實值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 168 samples, validate on 19 samples\n",
      "Epoch 1/300\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1051 - accuracy: 0.0060 - val_loss: 0.1316 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0283 - accuracy: 0.0060 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0112 - accuracy: 0.0060 - val_loss: 0.0413 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.0060 - val_loss: 0.0054 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 0.0060 - val_loss: 0.0081 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0048 - accuracy: 0.0060 - val_loss: 0.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0092 - accuracy: 0.0060 - val_loss: 0.0316 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0099 - accuracy: 0.0060 - val_loss: 0.0230 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0069 - accuracy: 0.0060 - val_loss: 0.0093 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/300\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 0.0036 - accuracy: 0.0060 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/300\n",
      "168/168 [==============================] - 0s 851us/step - loss: 0.0030 - accuracy: 0.0060 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/300\n",
      "168/168 [==============================] - 0s 888us/step - loss: 0.0045 - accuracy: 0.0060 - val_loss: 0.0052 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/300\n",
      "168/168 [==============================] - 0s 862us/step - loss: 0.0046 - accuracy: 0.0060 - val_loss: 0.0026 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/300\n",
      "168/168 [==============================] - 0s 835us/step - loss: 0.0030 - accuracy: 0.0060 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/300\n",
      "168/168 [==============================] - 0s 912us/step - loss: 0.0023 - accuracy: 0.0060 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/300\n",
      "168/168 [==============================] - 0s 861us/step - loss: 0.0028 - accuracy: 0.0060 - val_loss: 0.0055 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/300\n",
      "168/168 [==============================] - 0s 902us/step - loss: 0.0032 - accuracy: 0.0060 - val_loss: 0.0048 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/300\n",
      "168/168 [==============================] - 0s 776us/step - loss: 0.0030 - accuracy: 0.0060 - val_loss: 0.0026 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/300\n",
      "168/168 [==============================] - 0s 853us/step - loss: 0.0023 - accuracy: 0.0060 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/300\n",
      "168/168 [==============================] - 0s 864us/step - loss: 0.0021 - accuracy: 0.0060 - val_loss: 0.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0023 - accuracy: 0.0060 - val_loss: 0.0026 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/300\n",
      "168/168 [==============================] - 0s 841us/step - loss: 0.0025 - accuracy: 0.0060 - val_loss: 0.0021 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/300\n",
      "168/168 [==============================] - 0s 847us/step - loss: 0.0022 - accuracy: 0.0060 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/300\n",
      "168/168 [==============================] - 0s 892us/step - loss: 0.0021 - accuracy: 0.0060 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/300\n",
      "168/168 [==============================] - 0s 839us/step - loss: 0.0021 - accuracy: 0.0060 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/300\n",
      "168/168 [==============================] - 0s 876us/step - loss: 0.0022 - accuracy: 0.0060 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/300\n",
      "168/168 [==============================] - 0s 806us/step - loss: 0.0021 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/300\n",
      "168/168 [==============================] - 0s 824us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0015 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/300\n",
      "168/168 [==============================] - 0s 843us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/300\n",
      "168/168 [==============================] - 0s 839us/step - loss: 0.0021 - accuracy: 0.0060 - val_loss: 0.0017 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/300\n",
      "168/168 [==============================] - 0s 827us/step - loss: 0.0021 - accuracy: 0.0060 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/300\n",
      "168/168 [==============================] - 0s 889us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/300\n",
      "168/168 [==============================] - 0s 881us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/300\n",
      "168/168 [==============================] - 0s 805us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/300\n",
      "168/168 [==============================] - 0s 940us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/300\n",
      "168/168 [==============================] - 0s 853us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/300\n",
      "168/168 [==============================] - 0s 849us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/300\n",
      "168/168 [==============================] - 0s 830us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/300\n",
      "168/168 [==============================] - 0s 808us/step - loss: 0.0020 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/300\n",
      "168/168 [==============================] - 0s 808us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/300\n",
      "168/168 [==============================] - 0s 799us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/300\n",
      "168/168 [==============================] - 0s 793us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/300\n",
      "168/168 [==============================] - 0s 844us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/300\n",
      "168/168 [==============================] - 0s 864us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/300\n",
      "168/168 [==============================] - 0s 900us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/300\n",
      "168/168 [==============================] - 0s 857us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/300\n",
      "168/168 [==============================] - 0s 797us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/300\n",
      "168/168 [==============================] - 0s 808us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/300\n",
      "168/168 [==============================] - 0s 790us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/300\n",
      "168/168 [==============================] - 0s 806us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/300\n",
      "168/168 [==============================] - 0s 857us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/300\n",
      "168/168 [==============================] - 0s 841us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 0s 833us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/300\n",
      "168/168 [==============================] - 0s 798us/step - loss: 0.0019 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/300\n",
      "168/168 [==============================] - 0s 850us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/300\n",
      "168/168 [==============================] - 0s 834us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/300\n",
      "168/168 [==============================] - 0s 845us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/300\n",
      "168/168 [==============================] - 0s 789us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/300\n",
      "168/168 [==============================] - 0s 820us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/300\n",
      "168/168 [==============================] - 0s 812us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/300\n",
      "168/168 [==============================] - 0s 790us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/300\n",
      "168/168 [==============================] - 0s 840us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/300\n",
      "168/168 [==============================] - 0s 799us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/300\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 0.01 - 0s 785us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/300\n",
      "168/168 [==============================] - 0s 817us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/300\n",
      "168/168 [==============================] - 0s 798us/step - loss: 0.0018 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/300\n",
      "168/168 [==============================] - 0s 846us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/300\n",
      "168/168 [==============================] - 0s 771us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/300\n",
      "168/168 [==============================] - 0s 763us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/300\n",
      "168/168 [==============================] - 0s 780us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/300\n",
      "168/168 [==============================] - 0s 786us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/300\n",
      "168/168 [==============================] - 0s 898us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/300\n",
      "168/168 [==============================] - 0s 810us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/300\n",
      "168/168 [==============================] - 0s 659us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/300\n",
      "168/168 [==============================] - 0s 679us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/300\n",
      "168/168 [==============================] - 0s 797us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/300\n",
      "168/168 [==============================] - 0s 697us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/300\n",
      "168/168 [==============================] - 0s 678us/step - loss: 0.0017 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/300\n",
      "168/168 [==============================] - 0s 676us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/300\n",
      "168/168 [==============================] - 0s 893us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/300\n",
      "168/168 [==============================] - 0s 877us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/300\n",
      "168/168 [==============================] - 0s 865us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/300\n",
      "168/168 [==============================] - 0s 846us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/300\n",
      "168/168 [==============================] - 0s 675us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/300\n",
      "168/168 [==============================] - 0s 664us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/300\n",
      "168/168 [==============================] - 0s 642us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/300\n",
      "168/168 [==============================] - 0s 792us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/300\n",
      "168/168 [==============================] - 0s 789us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/300\n",
      "168/168 [==============================] - 0s 854us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/300\n",
      "168/168 [==============================] - 0s 842us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/300\n",
      "168/168 [==============================] - 0s 838us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/300\n",
      "168/168 [==============================] - 0s 888us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/300\n",
      "168/168 [==============================] - 0s 815us/step - loss: 0.0016 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/300\n",
      "168/168 [==============================] - 0s 796us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/300\n",
      "168/168 [==============================] - 0s 796us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/300\n",
      "168/168 [==============================] - 0s 782us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/300\n",
      "168/168 [==============================] - 0s 804us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/300\n",
      "168/168 [==============================] - 0s 779us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/300\n",
      "168/168 [==============================] - 0s 826us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/300\n",
      "168/168 [==============================] - 0s 778us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/300\n",
      "168/168 [==============================] - 0s 806us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/300\n",
      "168/168 [==============================] - 0s 800us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 0s 840us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/300\n",
      "168/168 [==============================] - 0s 781us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/300\n",
      "168/168 [==============================] - 0s 716us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/300\n",
      "168/168 [==============================] - 0s 856us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/300\n",
      "168/168 [==============================] - 0s 795us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/300\n",
      "168/168 [==============================] - 0s 869us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/300\n",
      "168/168 [==============================] - 0s 832us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/300\n",
      "168/168 [==============================] - 0s 819us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/300\n",
      "168/168 [==============================] - 0s 785us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/300\n",
      "168/168 [==============================] - 0s 791us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/300\n",
      "168/168 [==============================] - 0s 870us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/300\n",
      "168/168 [==============================] - 0s 792us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/300\n",
      "168/168 [==============================] - 0s 835us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/300\n",
      "168/168 [==============================] - 0s 834us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/300\n",
      "168/168 [==============================] - 0s 805us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/300\n",
      "168/168 [==============================] - 0s 800us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/300\n",
      "168/168 [==============================] - 0s 799us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/300\n",
      "168/168 [==============================] - 0s 858us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/300\n",
      "168/168 [==============================] - 0s 779us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/300\n",
      "168/168 [==============================] - 0s 785us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/300\n",
      "168/168 [==============================] - 0s 791us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/300\n",
      "168/168 [==============================] - 0s 798us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/300\n",
      "168/168 [==============================] - 0s 819us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/300\n",
      "168/168 [==============================] - 0s 822us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/300\n",
      "168/168 [==============================] - 0s 790us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/300\n",
      "168/168 [==============================] - 0s 806us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/300\n",
      "168/168 [==============================] - 0s 793us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/300\n",
      "168/168 [==============================] - 0s 848us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/300\n",
      "168/168 [==============================] - 0s 793us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/300\n",
      "168/168 [==============================] - 0s 791us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/300\n",
      "168/168 [==============================] - 0s 788us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/300\n",
      "168/168 [==============================] - 0s 845us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/300\n",
      "168/168 [==============================] - 0s 800us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/300\n",
      "168/168 [==============================] - 0s 738us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/300\n",
      "168/168 [==============================] - 0s 793us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/300\n",
      "168/168 [==============================] - 0s 850us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0018 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/300\n",
      "168/168 [==============================] - 0s 839us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/300\n",
      "168/168 [==============================] - 0s 832us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/300\n",
      "168/168 [==============================] - 0s 841us/step - loss: 0.0015 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/300\n",
      "168/168 [==============================] - 0s 842us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/300\n",
      "168/168 [==============================] - 0s 793us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0014 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/300\n",
      "168/168 [==============================] - 0s 804us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/300\n",
      "168/168 [==============================] - 0s 830us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/300\n",
      "168/168 [==============================] - 0s 781us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/300\n",
      "168/168 [==============================] - 0s 839us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 9.9617e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/300\n",
      "168/168 [==============================] - 0s 842us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 9.9245e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/300\n",
      "168/168 [==============================] - 0s 807us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/300\n",
      "168/168 [==============================] - 0s 815us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/300\n",
      "168/168 [==============================] - 0s 834us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 0s 783us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/300\n",
      "168/168 [==============================] - 0s 803us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/300\n",
      "168/168 [==============================] - 0s 772us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/300\n",
      "168/168 [==============================] - 0s 788us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 9.9381e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/300\n",
      "168/168 [==============================] - 0s 799us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/300\n",
      "168/168 [==============================] - 0s 783us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/300\n",
      "168/168 [==============================] - 0s 797us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 9.9829e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/300\n",
      "168/168 [==============================] - 0s 842us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/300\n",
      "168/168 [==============================] - 0s 799us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/300\n",
      "168/168 [==============================] - 0s 840us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/300\n",
      "168/168 [==============================] - 0s 839us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/300\n",
      "168/168 [==============================] - 0s 838us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/300\n",
      "168/168 [==============================] - 0s 798us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 9.9434e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/300\n",
      "168/168 [==============================] - 0s 839us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/300\n",
      "168/168 [==============================] - ETA: 0s - loss: 9.5908e-04 - accuracy: 0.0000e+ - 0s 794us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/300\n",
      "168/168 [==============================] - 0s 717us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 9.9353e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/300\n",
      "168/168 [==============================] - 0s 803us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/300\n",
      "168/168 [==============================] - 0s 769us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/300\n",
      "168/168 [==============================] - 0s 799us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.8375e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/300\n",
      "168/168 [==============================] - 0s 772us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/300\n",
      "168/168 [==============================] - 0s 843us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/300\n",
      "168/168 [==============================] - 0s 805us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/300\n",
      "168/168 [==============================] - 0s 785us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/300\n",
      "168/168 [==============================] - 0s 778us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/300\n",
      "168/168 [==============================] - 0s 795us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0014 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/300\n",
      "168/168 [==============================] - 0s 787us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 9.6552e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/300\n",
      "168/168 [==============================] - 0s 814us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/300\n",
      "168/168 [==============================] - 0s 897us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/300\n",
      "168/168 [==============================] - 0s 855us/step - loss: 0.0013 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/300\n",
      "168/168 [==============================] - 0s 838us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.5167e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/300\n",
      "168/168 [==============================] - 0s 836us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/300\n",
      "168/168 [==============================] - 0s 753us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/300\n",
      "168/168 [==============================] - 0s 663us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.6822e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/300\n",
      "168/168 [==============================] - 0s 744us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/300\n",
      "168/168 [==============================] - 0s 711us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/300\n",
      "168/168 [==============================] - 0s 867us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/300\n",
      "168/168 [==============================] - 0s 835us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/300\n",
      "168/168 [==============================] - 0s 758us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.7204e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/300\n",
      "168/168 [==============================] - 0s 667us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/300\n",
      "168/168 [==============================] - 0s 680us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.5841e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/300\n",
      "168/168 [==============================] - 0s 660us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.6528e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/300\n",
      "168/168 [==============================] - 0s 690us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/300\n",
      "168/168 [==============================] - 0s 827us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.7168e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/300\n",
      "168/168 [==============================] - 0s 845us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.2012e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/300\n",
      "168/168 [==============================] - 0s 869us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/300\n",
      "168/168 [==============================] - 0s 833us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/300\n",
      "168/168 [==============================] - 0s 841us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.2732e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/300\n",
      "168/168 [==============================] - 0s 845us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/300\n",
      "168/168 [==============================] - 0s 839us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/300\n",
      "168/168 [==============================] - 0s 852us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.5217e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/300\n",
      "168/168 [==============================] - 0s 849us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.6301e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 0s 780us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/300\n",
      "168/168 [==============================] - 0s 778us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/300\n",
      "168/168 [==============================] - 0s 808us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/300\n",
      "168/168 [==============================] - 0s 872us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/300\n",
      "168/168 [==============================] - 0s 824us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.2931e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/300\n",
      "168/168 [==============================] - 0s 791us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/300\n",
      "168/168 [==============================] - 0s 801us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.2548e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/300\n",
      "168/168 [==============================] - 0s 841us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.9334e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/300\n",
      "168/168 [==============================] - 0s 888us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.4302e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/300\n",
      "168/168 [==============================] - 0s 708us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/300\n",
      "168/168 [==============================] - 0s 662us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/300\n",
      "168/168 [==============================] - 0s 741us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.3307e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/300\n",
      "168/168 [==============================] - 0s 710us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.9060e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/300\n",
      "168/168 [==============================] - 0s 766us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/300\n",
      "168/168 [==============================] - 0s 796us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/300\n",
      "168/168 [==============================] - 0s 796us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.8483e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/300\n",
      "168/168 [==============================] - 0s 800us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.8665e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/300\n",
      "168/168 [==============================] - 0s 841us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/300\n",
      "168/168 [==============================] - 0s 846us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/300\n",
      "168/168 [==============================] - 0s 836us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.9171e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.6306e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/300\n",
      "168/168 [==============================] - 0s 807us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0016 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/300\n",
      "168/168 [==============================] - 0s 793us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.8704e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/300\n",
      "168/168 [==============================] - 0s 786us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.6263e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/300\n",
      "168/168 [==============================] - 0s 850us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/300\n",
      "168/168 [==============================] - 0s 845us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/300\n",
      "168/168 [==============================] - 0s 835us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.7057e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/300\n",
      "168/168 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 0.0000e+ - 0s 774us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.9462e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/300\n",
      "168/168 [==============================] - 0s 875us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.8927e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/300\n",
      "168/168 [==============================] - 0s 874us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.7269e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/300\n",
      "168/168 [==============================] - 0s 815us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.8013e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/300\n",
      "168/168 [==============================] - 0s 795us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/300\n",
      "168/168 [==============================] - 0s 816us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.9056e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/300\n",
      "168/168 [==============================] - 0s 796us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.4414e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/300\n",
      "168/168 [==============================] - 0s 843us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.9202e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/300\n",
      "168/168 [==============================] - 0s 843us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.3485e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/300\n",
      "168/168 [==============================] - 0s 979us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.8287e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/300\n",
      "168/168 [==============================] - 0s 850us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.6775e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/300\n",
      "168/168 [==============================] - 0s 885us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.4891e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/300\n",
      "168/168 [==============================] - 0s 858us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/300\n",
      "168/168 [==============================] - 0s 659us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 9.8218e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/300\n",
      "168/168 [==============================] - 0s 885us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.5775e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 257/300\n",
      "168/168 [==============================] - 0s 835us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.5572e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 258/300\n",
      "168/168 [==============================] - 0s 829us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.9924e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 259/300\n",
      "168/168 [==============================] - 0s 811us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.9257e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 260/300\n",
      "168/168 [==============================] - 0s 787us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.2645e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 261/300\n",
      "168/168 [==============================] - 0s 854us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.2323e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 262/300\n",
      "168/168 [==============================] - 0s 833us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.6982e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 263/300\n",
      "168/168 [==============================] - 0s 799us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.4588e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 264/300\n",
      "168/168 [==============================] - 0s 788us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.9398e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 265/300\n",
      "168/168 [==============================] - ETA: 0s - loss: 9.0721e-04 - accuracy: 0.01 - 0s 800us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 266/300\n",
      "168/168 [==============================] - 0s 759us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.2210e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 267/300\n",
      "168/168 [==============================] - 0s 800us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.0953e-04 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268/300\n",
      "168/168 [==============================] - 0s 829us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 8.7465e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 269/300\n",
      "168/168 [==============================] - 0s 842us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.3278e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 270/300\n",
      "168/168 [==============================] - 0s 799us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.2687e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 271/300\n",
      "168/168 [==============================] - 0s 831us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.3554e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 272/300\n",
      "168/168 [==============================] - 0s 804us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.8300e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 273/300\n",
      "168/168 [==============================] - 0s 798us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.1097e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 274/300\n",
      "168/168 [==============================] - 0s 813us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.4053e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 275/300\n",
      "168/168 [==============================] - 0s 795us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 276/300\n",
      "168/168 [==============================] - 0s 790us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.3997e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 277/300\n",
      "168/168 [==============================] - 0s 837us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 7.9020e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 278/300\n",
      "168/168 [==============================] - 0s 862us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 0.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 279/300\n",
      "168/168 [==============================] - 0s 797us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 0.0012 - val_accuracy: 0.0000e+00\n",
      "Epoch 280/300\n",
      "168/168 [==============================] - 0s 817us/step - loss: 0.0012 - accuracy: 0.0060 - val_loss: 7.9028e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 281/300\n",
      "168/168 [==============================] - 0s 837us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.8198e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 282/300\n",
      "168/168 [==============================] - 0s 815us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.9773e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 283/300\n",
      "168/168 [==============================] - 0s 866us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 7.7707e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 284/300\n",
      "168/168 [==============================] - 0s 864us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.1221e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 285/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.5056e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 286/300\n",
      "168/168 [==============================] - 0s 806us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.5584e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 287/300\n",
      "168/168 [==============================] - 0s 813us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 7.7049e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 288/300\n",
      "168/168 [==============================] - 0s 801us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 9.2733e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 289/300\n",
      "168/168 [==============================] - 0s 803us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.4661e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 290/300\n",
      "168/168 [==============================] - 0s 836us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.4016e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 291/300\n",
      "168/168 [==============================] - 0s 826us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.1713e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 292/300\n",
      "168/168 [==============================] - 0s 781us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.8308e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 293/300\n",
      "168/168 [==============================] - 0s 843us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 0.0011 - val_accuracy: 0.0000e+00\n",
      "Epoch 294/300\n",
      "168/168 [==============================] - 0s 830us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 7.4978e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 295/300\n",
      "168/168 [==============================] - 0s 794us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 7.4747e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 296/300\n",
      "168/168 [==============================] - 0s 828us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.7715e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 297/300\n",
      "168/168 [==============================] - 0s 851us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.0124e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 298/300\n",
      "168/168 [==============================] - 0s 829us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 7.4438e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 299/300\n",
      "168/168 [==============================] - 0s 791us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.6437e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 300/300\n",
      "168/168 [==============================] - 0s 793us/step - loss: 0.0011 - accuracy: 0.0060 - val_loss: 8.3564e-04 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning:\n",
      "\n",
      "Glyph 38928 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning:\n",
      "\n",
      "Glyph 28204 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning:\n",
      "\n",
      "Glyph 25910 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning:\n",
      "\n",
      "Glyph 30436 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning:\n",
      "\n",
      "Glyph 20729 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning:\n",
      "\n",
      "Glyph 38928 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning:\n",
      "\n",
      "Glyph 28204 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning:\n",
      "\n",
      "Glyph 25910 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning:\n",
      "\n",
      "Glyph 30436 missing from current font.\n",
      "\n",
      "C:\\Users\\User\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning:\n",
      "\n",
      "Glyph 20729 missing from current font.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 主程式\n",
    "pd.options.mode.chained_assignment = None  #取消顯示pandas資料重設警告\n",
    "filename = 'twstockyear2020.csv'\n",
    "df = pd.read_csv(filename, encoding='big5')  #以pandas讀取檔案\n",
    "ddprice=pd.DataFrame(df['收盤價'])\n",
    "    \n",
    "train_x, train_y, test_x, test_y, scaler, x, y, reshaped_data =load_data(ddprice, sequence_length=10, split=0.8)\n",
    "# train_x 共 230*0.8=184 筆, test_x 共 230*0.2=46 筆\n",
    "#print(train_x.shape,train_y.shape) # (184,10,1) (184,1)\n",
    "#print(test_x.shape,test_y.shape)   # (46,10,1)  (46,1)\n",
    "predict_y, test_y = train_model(train_x, train_y, test_x, test_y)\n",
    "predict_y = scaler.inverse_transform([[i] for i in predict_y]) # 還原\n",
    "test_y = scaler.inverse_transform(test_y)  # 還原\n",
    "\n",
    "plt.plot(predict_y, 'b:') #預測\n",
    "plt.plot(test_y, 'r-')    #收盤價\n",
    "plt.legend(['預測', '收盤價'])\n",
    "plt.show()\n",
    "\n",
    "# 建立 DataFrame，加入 predict_y、test_y，準備以 plotly 繪圖\n",
    "dd2=pd.DataFrame({\"predict\":list(predict_y),\"label\":list(test_y)})\n",
    "#轉換為 numpy 陣列，並轉為 float\n",
    "dd2[\"predict\"] = np.array(dd2[\"predict\"]).astype('float64')\n",
    "dd2[\"label\"] = np.array(dd2[\"label\"]).astype('float64')\n",
    "\n",
    "data = [\n",
    "    Scatter(y=dd2[\"predict\"],name='預測'),\n",
    "    Scatter(y=dd2[\"label\"],name='收盤價')\n",
    "] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot({\"data\": data, \"layout\": Layout(title='2020年個股預測圖')},auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp37-cp37m-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn->sklearn) (1.6.0)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.1 scikit-learn-0.24.1 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from plotly) (1.15.0)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Building wheels for collected packages: retrying\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11429 sha256=89105ccb9271895cb1637923b7215de62cbc8543ac169294264cf0d4cf52bfe9\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\f9\\8d\\8d\\f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
      "Successfully built retrying\n",
      "Installing collected packages: retrying, plotly\n",
      "Successfully installed plotly-4.14.3 retrying-1.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
